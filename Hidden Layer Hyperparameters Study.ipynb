{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.colors as pc\n",
    "import plotly.subplots as sp\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, width: int, depth: int, act, initialise = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        funcs = {\"relu\": nn.ReLU, \n",
    "                 \"selu\": nn.SELU, \n",
    "                 \"tanh\": nn.Tanh, \n",
    "                 \"sigmoid\": nn.Sigmoid, \n",
    "                 \"leakyrelu\": nn.LeakyReLU}\n",
    "\n",
    "        # activation function\n",
    "        activation = funcs[act]\n",
    "        \n",
    "        \n",
    "        # input layer\n",
    "        self.input = nn.Linear(3, depth)\n",
    "        if initialise:\n",
    "            init.kaiming_normal_(self.input.weight,nonlinearity=act)\n",
    "\n",
    "        # hidden layers\n",
    "        self.hidden = nn.Sequential(\n",
    "            *[\n",
    "                nn.Sequential(nn.Linear(depth, depth), activation())\n",
    "                for _ in range(width - 2)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for layer in self.hidden.children():\n",
    "            if isinstance(layer, nn.Linear) and initialise:\n",
    "                init.kaiming_normal_(layer.weight,nonlinearity=act)\n",
    "\n",
    "        # output layer\n",
    "        self.output = nn.Linear(depth, 1)\n",
    "\n",
    "        self.constants()\n",
    "\n",
    "    def constants(self) -> None:\n",
    "        self.L: float = 0.1  # m\n",
    "        self.sigma: float = 0.5\n",
    "\n",
    "        self.k: int = 15  # W/mK\n",
    "\n",
    "        self.ht: int = 200\n",
    "        self.hb: int = 100\n",
    "\n",
    "        self.Bi_t: float = self.ht * self.L / self.k\n",
    "        self.Bi_b: float = self.hb * self.L / self.k\n",
    "\n",
    "        self.lossTotal: list[float] = [1]\n",
    "        self.epochData: dict[int, np.ndarray] = {}\n",
    "        \n",
    "        self.Xpoints: int = 21\n",
    "        self.Ypoints: int = 11\n",
    "        self.Tpoints: int = 6\n",
    "        \n",
    "        self.Tmax: float = 5.0\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, y: torch.Tensor, t: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        inp = torch.cat((x, y, t), dim=1)\n",
    "        inputted = self.input(inp)\n",
    "        hidden = self.hidden(inputted)\n",
    "        out = self.output(hidden)\n",
    "        return out\n",
    "\n",
    "    def init_boundaries(self, N: int) -> None:\n",
    "\n",
    "        # Defines a vertical tensor of -sigma and sigma y values, then extends the vector to a 3D tensor for\n",
    "        # boundary training then reshapes it to 4D column tensor for inputting into the network\n",
    "        self.y0_boundary: torch.Tensor = (\n",
    "            torch.full((N, 1), -self.sigma)\n",
    "            .expand(N, N, N)\n",
    "            .reshape(N**3, 1)\n",
    "            .requires_grad_(True)\n",
    "            .to(device)\n",
    "        )\n",
    "        self.y1_boundary: torch.Tensor = (\n",
    "            torch.full((N, 1), self.sigma)\n",
    "            .expand(N, N, N)\n",
    "            .reshape(N**3, 1)\n",
    "            .requires_grad_(True)\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        # Same operations as above but for random values in the range of -sigma and sigma for initial condition\n",
    "        self.y_rand: torch.Tensor = (\n",
    "            (-2 * self.sigma * torch.rand(N, 1) + self.sigma)\n",
    "            .expand(N, N, N)\n",
    "            .reshape(N**3, 1)\n",
    "            .requires_grad_(True)\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        # Defines a vertical tensor of -1 and 1 x values, then extends the vector to a 3D tensor for\n",
    "        # boundary training then reshapes it to 4D column tensor for inputting into the network\n",
    "        self.x0_boundary: torch.Tensor = (\n",
    "            torch.full((N,), -1.0)\n",
    "            .expand(N, N, N)\n",
    "            .reshape(N**3, 1)\n",
    "            .requires_grad_(True)\n",
    "            .to(device)\n",
    "        )\n",
    "        self.x1_boundary: torch.Tensor = (\n",
    "            torch.full((N,), 1.0)\n",
    "            .expand(N, N, N)\n",
    "            .reshape(N**3, 1)\n",
    "            .requires_grad_(True)\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        # Same operations as above but for random values in the range of -1 and 1 for initial condition\n",
    "        self.x_rand: torch.Tensor = (\n",
    "            (-2 * torch.rand(N) + 1.0)\n",
    "            .expand(N, N, N)\n",
    "            .reshape(N**3, 1)\n",
    "            .requires_grad_(True)\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        # Defines a vertical tensor of 0 t values, then extends the vector to a 3D tensor for\n",
    "        # initial condition training then reshapes it to 4D column tensor for inputting into the network\n",
    "        self.t0_boundary: torch.Tensor = (\n",
    "            torch.tensor(0.0)\n",
    "            .expand(N, N, N)\n",
    "            .reshape(N**3, 1)\n",
    "            .requires_grad_(True)\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "        # Same operations as above but for random values in the range of 0 and 10 for boundary training\n",
    "        self.t_rand: torch.Tensor = (\n",
    "            (self.Tmax*torch.rand(N, 1, 1))\n",
    "            .expand(N, N, N)\n",
    "            .reshape(N**3, 1)\n",
    "            .requires_grad_(True)\n",
    "            .to(device)\n",
    "        )\n",
    "\n",
    "    def derivative(self, f: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.autograd.grad(\n",
    "            f, x, grad_outputs=torch.ones_like(f).to(device), create_graph=True, allow_unused= True\n",
    "        )[0]\n",
    "\n",
    "    def losses(self, N: int) -> torch.Tensor:\n",
    "\n",
    "        self.init_boundaries(N)\n",
    "\n",
    "        # -1x all y values boundary condition is cooling heat transfer in the y direction\n",
    "        T: torch.Tensor = self.forward(self.x0_boundary, self.y_rand, self.t_rand)\n",
    "        T_x0: torch.Tensor = self.derivative(T, self.x0_boundary)\n",
    "\n",
    "        minusx_loss: torch.Tensor = torch.mean((T_x0 - self.Bi_b * T) ** 2)\n",
    "\n",
    "        # 1x all y values boundary condition is heating heat transfer in the y direction\n",
    "        T2: torch.Tensor = self.forward(self.x1_boundary, self.y_rand, self.t_rand)\n",
    "        T_x1: torch.Tensor = self.derivative(T2, self.x1_boundary)\n",
    "\n",
    "        x_loss: torch.Tensor = torch.mean(\n",
    "            (T_x1 + self.Bi_t * (T2 - 1)) ** 2\n",
    "        )\n",
    "\n",
    "        # -sigma y all x values boundary condition is adiabatic in the x direction\n",
    "        T3: torch.Tensor = self.forward(self.x_rand, self.y0_boundary, self.t_rand)\n",
    "        T_y0: torch.Tensor = self.derivative(T3, self.y0_boundary)\n",
    "\n",
    "        minusy_loss: torch.Tensor = torch.mean(T_y0**2)\n",
    "\n",
    "        # sigma y all x values boundary condition is adiabatic in the x direction\n",
    "        T4: torch.Tensor = self.forward(self.x_rand, self.y1_boundary, self.t_rand)\n",
    "        T_y1: torch.Tensor = self.derivative(T4, self.y1_boundary)\n",
    "\n",
    "        y_loss: torch.Tensor = torch.mean(T_y1**2)\n",
    "\n",
    "        # loss for initial condition\n",
    "\n",
    "        T_IC: torch.Tensor = self.forward(self.x_rand, self.y_rand, self.t0_boundary)\n",
    "        t0_loss: torch.Tensor = torch.mean((T_IC - 1) ** 2)\n",
    "\n",
    "        # loss for physics sample\n",
    "        T_phy: torch.Tensor = self.forward(self.x_rand, self.y_rand, self.t_rand)\n",
    "        \n",
    "        T_x_phy: torch.Tensor = self.derivative(T_phy, self.x_rand)\n",
    "        T_xx_phy: torch.Tensor = self.derivative(T_x_phy, self.x_rand)\n",
    "        \n",
    "        T_y_phy: torch.Tensor = self.derivative(T_phy, self.y_rand)\n",
    "        T_yy_phy: torch.Tensor = self.derivative(T_y_phy, self.y_rand)\n",
    "        \n",
    "        T_t_phy: torch.Tensor = self.derivative(T_phy, self.t_rand)\n",
    "\n",
    "        phys_loss: torch.Tensor = torch.mean(\n",
    "            (T_t_phy - 4 * (T_xx_phy + (T_yy_phy / (self.sigma**2) )) ) ** 2\n",
    "        )\n",
    "\n",
    "        loss: torch.Tensor = (\n",
    "            minusx_loss + x_loss + minusy_loss + y_loss + t0_loss + phys_loss\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def train(\n",
    "        self, epochs_max: int, N: int, optimiser: str = \"LBFGS\", lr: float = 1\n",
    "    ) -> None:\n",
    "\n",
    "        def closure() -> float:\n",
    "            optimizer.zero_grad()\n",
    "            loss: torch.Tensor = self.losses(N)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(self.parameters(), clip_value=1.0)\n",
    "            return loss\n",
    "\n",
    "        loss_val: float = self.lossTotal[-1]\n",
    "        epoch: int = len(self.lossTotal) - 1\n",
    "        epoch_times: list[float] = []\n",
    "        window_size: int = 10\n",
    "\n",
    "        scaler = GradScaler()\n",
    "\n",
    "        if optimiser == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        elif optimiser == \"LBFGS\":\n",
    "            optimizer = torch.optim.LBFGS(\n",
    "                self.parameters(),\n",
    "                max_iter=100,\n",
    "                history_size=100,\n",
    "                line_search_fn=\"strong_wolfe\",\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid optimiser\")\n",
    "\n",
    "        start: float = time.time()\n",
    "\n",
    "        while epoch < epochs_max and loss_val > 1e-6:\n",
    "            epoch += 1\n",
    "            if optimiser == \"Adam\":\n",
    "                optimizer.zero_grad()\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss: torch.Tensor = self.losses(N)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                torch.nn.utils.clip_grad_value_(\n",
    "                    self.parameters(), clip_value=1.0\n",
    "                )  # clip the unscaled gradients\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss: torch.Tensor = optimizer.step(closure)\n",
    "\n",
    "            loss_val: float = loss.item()\n",
    "            self.lossTotal.append(loss_val)\n",
    "\n",
    "            # compute epoch time\n",
    "            epoch_time: float = time.time() - start\n",
    "            epoch_times.append(epoch_time)\n",
    "            start: float = time.time()  # Reset the start time for the next epoch\n",
    "\n",
    "            # Compute the moving average of epoch times\n",
    "            window_times: list[float] = epoch_times[-window_size:]\n",
    "            avg_epoch_time: float = sum(window_times) / len(window_times)\n",
    "\n",
    "            # Calculate the remaining time\n",
    "            time_remaining: float = (epochs_max - epoch) * avg_epoch_time\n",
    "\n",
    "            # print loss and parameter values\n",
    "            print(\n",
    "                f'Epoch: {epoch}  Loss: {loss_val:.8f} Time/Epoch: {avg_epoch_time:.4f}s ETA: {time.strftime(\"%H Hours %M Minutes and %S Seconds\", time.gmtime(time_remaining))}',\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "        \n",
    "        self.time = epoch_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Width: 7  Depth: 512\n",
      "Epoch: 4473  Loss: 0.00083721 Time/Epoch: 28.3831s ETA: 19 Hours 34 Minutes and 33 Secondss\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m pinn \u001b[38;5;241m=\u001b[39m Network(\u001b[38;5;28mint\u001b[39m(width), \u001b[38;5;28mint\u001b[39m(depth),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Train the network for 20000 epochs\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m pinn\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Store the loss value in the dictionary\u001b[39;00m\n\u001b[0;32m     33\u001b[0m size_loss_values[width][depth] \u001b[38;5;241m=\u001b[39m pinn\u001b[38;5;241m.\u001b[39mlossTotal\n",
      "Cell \u001b[1;32mIn[2], line 254\u001b[0m, in \u001b[0;36mNetwork.train\u001b[1;34m(self, epochs_max, N, optimiser, lr)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m    252\u001b[0m     loss: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses(N)\n\u001b[1;32m--> 254\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    255\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)  \u001b[38;5;66;03m# unscale the gradients of optimizer's assigned params in-place\u001b[39;00m\n\u001b[0;32m    256\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_value_(\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), clip_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    258\u001b[0m )  \u001b[38;5;66;03m# clip the unscaled gradients\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aryam\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\aryam\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gc\n",
    "# Define the range of network sizes to explore\n",
    "width_range = [\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"] \n",
    "depth_range = [\"2\", \"4\", \"8\", \"16\", \"32\", \"64\", \"128\", \"256\",\"512\"] #1024 fills ram\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# nn seed\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with open('size_loss.json') as f:\n",
    "    size_loss_values = json.load(f)\n",
    "    \n",
    "with open('size_time.json') as f:\n",
    "    size_time_values = json.load(f)\n",
    "\n",
    "# Perform the parameter space study\n",
    "for width in width_range:\n",
    "    for depth in depth_range:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        if size_loss_values.get(width) is not None and size_loss_values[width].get(depth) is not None:\n",
    "            continue\n",
    "        print(f\"\\nWidth: {width}  Depth: {depth}\")\n",
    "        # Create a new instance of the Network class with the specified width and depth\n",
    "        pinn = Network(int(width), int(depth),'selu', True).to(device)\n",
    "        \n",
    "        # Train the network for 20000 epochs\n",
    "        pinn.train(10000, 50, 'Adam', 1e-4)\n",
    "        \n",
    "        # Store the loss value in the dictionary\n",
    "        \n",
    "        size_loss_values[width][depth] = pinn.lossTotal\n",
    "        size_time_values[width][depth] = pinn.time\n",
    "        del pinn\n",
    "        with open('size_loss.json', 'w') as f:\n",
    "            json.dump(size_loss_values, f)\n",
    "            \n",
    "        with open('size_time.json', 'w') as f:\n",
    "            json.dump(size_time_values, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "lossjson = {3: {depth: size_loss_values.get((3, depth),[]) for depth in depth_range } }\n",
    "timejson = {3: {depth: size_time_values.get((3, depth),[]) for depth in depth_range } }\n",
    "\n",
    "# Save size_loss_values dictionary as JSON\n",
    "with open('size_loss.json', 'w') as file:\n",
    "\n",
    "# Save size_time_values dictionary as JSON\n",
    "with open('size_time.json', 'w') as file:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
